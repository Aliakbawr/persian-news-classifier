# -*- coding: utf-8 -*-
"""news-classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LG13yItgi94O5u6zt7HB80yEy5pQiWPe

Load Libraries
"""

import pandas as pd
import re
from hazm import Normalizer, word_tokenize, stopwords_list, Lemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

""" Load and preprocess dataset"""

df = pd.read_csv("/content/drive/MyDrive/projects/Persian-classifier/data/main_news.csv")
print(df.shape)
print(df.info())
df.head()

df.drop(columns=['Unnamed: 0', 'image', 'link', 'site'], inplace=True)
df['text'] = df['title'] + ' ' + df['description'] #Combine title and description
df.drop(columns=['title', 'description'], inplace=True)
df.dropna(inplace=True) # Remove missing values

print(df.isnull().sum())
print(df['tags'].value_counts())

"""Simplify and remap labels"""

new_label_map = {
    'فرهنگ و هنر': 'فرهنگ', 'فرهنگی': 'فرهنگ', 'فرهنگی و هنری': 'فرهنگ',
    'اقتصاد': 'اقتصاد', 'اقتصاد ایران': 'اقتصاد', 'اقتصادی': 'اقتصاد', 'بازار': 'اقتصاد',
    'ورزشی': 'ورزش', 'ورزش': 'ورزش',
    'ایران': 'سیاست', 'ايران': 'سیاست', 'سیاسی': 'سیاست',
    'جهان': 'بین‌الملل', 'بین الملل': 'بین‌الملل', 'الشرق الأوسط': 'بین‌الملل',
    'اجتماعی': 'اجتماعی', 'شبکه‌های اجتماعی': 'اجتماعی', 'استانها': 'اجتماعی', 'استان‌ها': 'اجتماعی',
    'دانش و فناوری': 'تکنولوژی', 'دانش و محیط زیست': 'تکنولوژی', 'علمی و دانشگاهی': 'تکنولوژی',
    'آلمان': 'بین‌الملل', 'دویچه وله': 'بین‌الملل', 'دوره‌های زبان آلمانی': 'بین‌الملل', 'آلمانی پیش‌رفته': 'بین‌الملل',
    'خبرخوان': 'بایگانی', 'NRS-Import': 'بایگانی', 'عکس': 'بایگانی', 'رسانه‌ها': 'بایگانی',
    'گرافیک': 'بایگانی', 'ایسنا+': 'بایگانی', 'گوناگون': 'بایگانی', 'ویدئو': 'بایگانی', 'دیدگاه': 'بایگانی',
    'Community D': 'بایگانی'
}

def simplify_label(label):
    return label.split('>')[0].strip()

def remap_label(label):
    return new_label_map.get(label, 'بایگانی')

df['tags'] = df['tags'].apply(simplify_label).apply(remap_label)

"""Split into main (non-baygani) and baygani datasets"""

df_main = df[df['tags'] != 'بایگانی'].copy()
df_baygani = df[df['tags'] == 'بایگانی'].copy()

"""Text preprocessing: normalization, tokenization, stopword removal, lemmatization"""

normalizer = Normalizer()
lemmatizer = Lemmatizer()
stopwords = set(stopwords_list())
custom_stopwords = {'ایران', 'کشور', 'تهران', 'گزارش', 'خبر', 'امروز', 'دیروز'}
stopwords.update(custom_stopwords)

def clean_text(text):
    text = normalizer.normalize(text)
    text = re.sub(r'<[^>]+>|http\S+|www\S+|[A-Za-z0-9%_=/<>:";!?()\[\]{}|@#$^&*~`+\\\-]|[^\u0600-\u06FF\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]
    return ' '.join(tokens)

df_main['text_clean'] = df_main['text'].apply(clean_text)
df_baygani['text_clean'] = df_baygani['text'].apply(clean_text)

"""Initial model to reclassify baygani samples"""

vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2)
X_main = vectorizer.fit_transform(df_main['text_clean'])
y_main = df_main['tags']
X_train, X_test, y_train, y_test = train_test_split(X_main, y_main, test_size=0.2, random_state=42, stratify=y_main)
svc = LinearSVC(class_weight='balanced', max_iter=10000, C=1.0, random_state=42)
svc.fit(X_train, y_train)
print("Initial Model Performance:\n", classification_report(y_test, svc.predict(X_test)))

"""Reclassify baygani samples"""

X_baygani = vectorizer.transform(df_baygani['text_clean'])
df_baygani['tags'] = svc.predict(X_baygani)

"""Combine datasets"""

df_final = pd.concat([df_main, df_baygani], ignore_index=True)
df_final.to_csv("news_dataset_with_reclassified_baygani.csv", index=False)
print("Final Dataset Shape:", df_final.shape)
print("Label Distribution:\n", df_final['tags'].value_counts())

""" Final vectorization"""

vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.85)
X = vectorizer.fit_transform(df_final['text_clean'])
y = df_final['tags']

"""Balance classes with SMOTE"""

class_counts = y.value_counts()
minority_classes = class_counts[class_counts < 2000].index
sampling_strategy = {cls: 2000 for cls in minority_classes}
smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
print("Shape after SMOTE:", X_resampled.shape)
print("Resampled Label Distribution:\n", pd.Series(y_resampled).value_counts())

"""Train-test split"""

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

"""Model training with hyperparameter tuning"""

param_grid_svc = {'C': [0.8, 1, 1.2]}
grid_svc = GridSearchCV(LinearSVC(class_weight='balanced', max_iter=10000, random_state=42), param_grid_svc, cv=5, scoring='accuracy', n_jobs=-1)
grid_svc.fit(X_train, y_train)
best_model = grid_svc.best_estimator_
print("Best LinearSVC Parameters:", grid_svc.best_params_)
print("Best LinearSVC CV Score:", grid_svc.best_score_)

"""Evaluation"""

y_pred = best_model.predict(X_test)
report = classification_report(y_test, y_pred, output_dict=True)
print("\nClassification Report:\n", pd.DataFrame(report).transpose())

"""Confusion matrix"""

labels = sorted(df_final['tags'].unique())
cm = confusion_matrix(y_test, y_pred, labels=labels)
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()

""" F1-scores per class"""

f1_scores = {k: v['f1-score'] for k, v in report.items() if k in labels}
plt.figure(figsize=(12, 6))
sns.barplot(x=list(f1_scores.keys()), y=list(f1_scores.values()), hue=list(f1_scores.keys()), palette='viridis', legend=False)
plt.title('F1-Score per Class')
plt.xlabel('Class Labels')
plt.ylabel('F1-Score')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('f1_scores.png')
plt.show()

"""Print key metrics"""

print(f"\nAccuracy: {report['accuracy']:.2f}")
print(f"Macro Avg F1-score: {report['macro avg']['f1-score']:.2f}")
print(f"Weighted Avg F1-score: {report['weighted avg']['f1-score']:.2f}")